{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN8iYqRR95joCkl7UHT324N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Black Taxis"],"metadata":{"id":"FOchP94YTxRg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_sXyU1rS_pH"},"outputs":[],"source":["import io\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","\n","\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n","    response = requests.get(url)\n","\n","    # Contenido de la pagina\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    # Creo una lista para alojar todos los links que sean de los archivos parquet\n","    parquet_files = []\n","\n","    # Hago una filtración de los archivos que pertenecen solamente a los años que nos importan\n","    selected_years = ['2020', '2021', '2022', '2023']\n","    years = [2020, 2021, 2022, 2023]\n","\n","    # Busco todos los elementos que sean links\n","    for link in soup.find_all('a',href=True):\n","        if link['href'].endswith('.parquet'): # Si el elemento termina en .parquet, añado el elemento a la lista parquet_files\n","            if any(year in link['href'] for year in selected_years):\n","                print(link['href'])\n","                parquet_files.append(link['href'])\n","\n","    high_volume_fhv = []\n","    for link in parquet_files:\n","        if 'fhvhv_tripdata' in link: # Todos los links que contengan 'yellow_tripdata' en su texto\n","            high_volume_fhv.append(link)\n","\n","    high_volume_fhv = sorted(high_volume_fhv, reverse=True)\n","\n","    # IdLocation de Manhattan\n","    manhattan_zones = [  4,  12,  13,  24,  41,  42,  43,  45,  48,  50,  68,  74,  75,\n","        79,  87,  88,  90, 100, 103, 104, 105, 107, 113, 114, 116, 120,\n","       125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153,\n","       158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211, 224,\n","       229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246,\n","       249, 261, 262, 263]\n","\n","    # Creo la lista para ir depositando todos los dataframes\n","    dataframes = []\n","    for parquet_file in high_volume_fhv:\n","        df = pd.read_parquet(parquet_file)\n","        df = df[['pickup_datetime','PULocationID','DOLocationID']]\n","        df = df[df['pickup_datetime'].dt.year.isin(years)].reset_index(drop=True)\n","\n","        # Selecciono solamente los registros que pertenecen a viajes del distrito de Manhattan\n","        df = df[df.PULocationID.isin(manhattan_zones) & df.DOLocationID.isin(manhattan_zones)].reset_index(drop=True)\n","        dataframes.append(df)\n","\n","    # Ahora concateno todos esos dataframes en uno solo\n","    full_df = pd.concat(dataframes, ignore_index=True)\n","    '''\n","    full_df['idblack'] = ('black' + full_df['pickup_datetime'].dt.year.astype(str) + full_df['pickup_datetime'].dt.strftime('%m%d%H%M%S'))\n","\n","    full_df['idblack'] = full_df['idblack'].str.replace(':','')\n","\n","    full_df = full_df[['idblack',\n","    'pickup_datetime',\n","    'PULocationID',\n","    'DOLocationID']]\n","    '''\n","\n","    return full_df\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Template code for testing the output of the block.\n","    \"\"\"\n","    assert output is not None, 'The output is undefined'"]},{"cell_type":"markdown","source":["# Yellow Taxis"],"metadata":{"id":"tXh-ygqYT9cA"}},{"cell_type":"code","source":["import io\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","\n","\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n","    response = requests.get(url)\n","\n","    # Contenido de la pagina\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    # Creo una lista para alojar todos los links que sean de los archivos parquet\n","    parquet_files = []\n","\n","    # Hago una filtración de los archivos que pertenecen solamente a los años que nos importan\n","\n","    selected_years = ['2020', '2021', '2022', '2023']\n","    years = [2020, 2021, 2022, 2023]\n","\n","    # Busco todos los elementos que sean links\n","    for link in soup.find_all('a',href=True):\n","        if link['href'].endswith('.parquet'): # Si el elemento termina en .parquet, añado el elemento a la lista parquet_files\n","            if any(year in link['href'] for year in selected_years):\n","                print(link['href'])\n","                parquet_files.append(link['href'])\n","\n","    yellow_taxis = []\n","    for link in parquet_files:\n","        if 'yellow_tripdata' in link: # Todos los links que contengan 'yellow_tripdata' en su texto\n","            yellow_taxis.append(link)\n","\n","    yellow_taxis = sorted(yellow_taxis, reverse=True)\n","\n","    # IdLocation de Manhattan\n","    manhattan_zones = [  4,  12,  13,  24,  41,  42,  43,  45,  48,  50,  68,  74,  75,\n","        79,  87,  88,  90, 100, 103, 104, 105, 107, 113, 114, 116, 120,\n","       125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153,\n","       158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211, 224,\n","       229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246,\n","       249, 261, 262, 263]\n","\n","    # Creo la lista para ir depositando todos los dataframes\n","    dataframes = []\n","    for parquet_file in yellow_taxis:\n","        df = pd.read_parquet(parquet_file)\n","        df = df[['tpep_pickup_datetime','PULocationID','DOLocationID']]\n","        df = df[df['tpep_pickup_datetime'].dt.year.isin(years)].reset_index(drop=True)\n","\n","        # Selecciono solamente los registros que pertenecen a viajes del distrito de Manhattan\n","        df = df[df.PULocationID.isin(manhattan_zones) & df.DOLocationID.isin(manhattan_zones)].reset_index(drop=True)\n","        dataframes.append(df)\n","\n","    # Ahora concateno todos esos dataframes en uno solo\n","    full_df = pd.concat(dataframes, ignore_index=True)\n","\n","    return full_df\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Template code for testing the output of the block.\n","    \"\"\"\n","    assert output is not None, 'The output is undefined'"],"metadata":{"id":"thnN6UoBT9Pp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Grey Taxis"],"metadata":{"id":"c7s_VJi3T_cI"}},{"cell_type":"code","source":["list(map(lambda x: str(x), range(2017,2020)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"npQqPuho7rym","executionInfo":{"status":"ok","timestamp":1693507870545,"user_tz":180,"elapsed":250,"user":{"displayName":"Uriel Mendez","userId":"00508324525312336488"}},"outputId":"797fdd23-2c01-418d-b4d2-269cbb0c53a3"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['2017', '2018', '2019']"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["import io\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n","    response = requests.get(url)\n","\n","    # Contenido de la pagina\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    # Creo una lista para alojar todos los links que sean de los archivos parquet\n","    parquet_files = []\n","\n","    # Hago una filtración de los archivos que pertenecen solamente a los años que nos importan\n","\n","    selected_years = list(map(lambda x: str(x),range(2017,2020)))\n","    years = list(range(2017,2020))\n","\n","    # Busco todos los elementos que sean links\n","    for link in soup.find_all('a',href=True):\n","        if link['href'].endswith('.parquet'): # Si el elemento termina en .parquet, añado el elemento a la lista parquet_files\n","            if any(year in link['href'] for year in selected_years):\n","                print(link['href'])\n","                parquet_files.append(link['href'])\n","\n","    for_hire_vehicles = []\n","    for link in parquet_files:\n","        if 'fhv_tripdata' in link: # Todos los links que contengan 'yellow_tripdata' en su texto\n","            for_hire_vehicles.append(link)\n","\n","    for_hire_vehicles = sorted(for_hire_vehicles, reverse=True)\n","\n","    # IdLocation de Manhattan\n","    manhattan_zones = [  4,  12,  13,  24,  41,  42,  43,  45,  48,  50,  68,  74,  75,\n","        79,  87,  88,  90, 100, 103, 104, 105, 107, 113, 114, 116, 120,\n","       125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153,\n","       158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211, 224,\n","       229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246,\n","       249, 261, 262, 263]\n","\n","    # Creo la lista para ir depositando todos los dataframes\n","    dataframes = []\n","    for parquet_file in for_hire_vehicles:\n","        df = pd.read_parquet(parquet_file)\n","        df = df[['pickup_datetime','PUlocationID','DOlocationID']]\n","        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n","        df = df[df['pickup_datetime'].dt.year.isin(years)].reset_index(drop=True)\n","\n","        # Selecciono solamente los registros que pertenecen a viajes del distrito de Manhattan\n","        df = df[df.PUlocationID.isin(manhattan_zones) & df.DOlocationID.isin(manhattan_zones)].reset_index(drop=True)\n","        dataframes.append(df)\n","\n","    # Ahora concateno todos esos dataframes en uno solo\n","    full_df = pd.concat(dataframes, ignore_index=True)\n","    full_df['idgrey'] = ('grey' + full_df['pickup_datetime'].dt.year.astype(str) + full_df['pickup_datetime'].dt.strftime('%m%d%H%M%S'))\n","\n","    full_df['idgrey'] = full_df['idgrey'].str.replace(':','')\n","\n","    full_df = full_df[['idgrey',\n","    'pickup_datetime',\n","    'PUlocationID',\n","    'DOlocationID']]\n","\n","    return full_df\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Template code for testing the output of the block.\n","    \"\"\"\n","    assert output is not None, 'The output is undefined'"],"metadata":{"id":"Xm0sMeuHUBFl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Green Taxis"],"metadata":{"id":"AVwNjLb_UBYR"}},{"cell_type":"code","source":["import io\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","\n","\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n","    response = requests.get(url)\n","\n","    # Contenido de la pagina\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    # Creo una lista para alojar todos los links que sean de los archivos parquet\n","    parquet_files = []\n","\n","    # Hago una filtración de los archivos que pertenecen solamente a los años que nos importan\n","    selected_years = ['2020', '2021', '2022', '2023']\n","    years = [2020, 2021, 2022, 2023]\n","\n","    # Busco todos los elementos que sean links\n","    for link in soup.find_all('a',href=True):\n","        if link['href'].endswith('.parquet'): # Si el elemento termina en .parquet, añado el elemento a la lista parquet_files\n","            if any(year in link['href'] for year in selected_years):\n","                print(link['href'])\n","                parquet_files.append(link['href'])\n","\n","    green_taxis = []\n","    for link in parquet_files:\n","        if 'green_tripdata' in link: # Todos los links que contengan 'yellow_tripdata' en su texto\n","            green_taxis.append(link)\n","\n","    green_taxis = sorted(green_taxis, reverse=True)\n","\n","    # IdLocation de Manhattan\n","    manhattan_zones = [  4,  12,  13,  24,  41,  42,  43,  45,  48,  50,  68,  74,  75,\n","        79,  87,  88,  90, 100, 103, 104, 105, 107, 113, 114, 116, 120,\n","       125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153,\n","       158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211, 224,\n","       229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246,\n","       249, 261, 262, 263]\n","\n","    # Creo la lista para ir depositando todos los dataframes\n","    dataframes = []\n","    for parquet_file in green_taxis:\n","        df = pd.read_parquet(parquet_file)\n","        df = df[['lpep_pickup_datetime','PULocationID','DOLocationID']]\n","        df = df[df['lpep_pickup_datetime'].dt.year.isin(years)].reset_index(drop=True)\n","\n","        # Selecciono solamente los registros que pertenecen a viajes del distrito de Manhattan\n","        df = df[df.PULocationID.isin(manhattan_zones) & df.DOLocationID.isin(manhattan_zones)].reset_index(drop=True)\n","        dataframes.append(df)\n","\n","    # Ahora concateno todos esos dataframes en uno solo\n","    full_df = pd.concat(dataframes, ignore_index=True)\n","\n","    full_df['idgreen'] = ('green' + full_df['lpep_pickup_datetime'].dt.year.astype(str) + full_df['lpep_pickup_datetime'].dt.strftime('%m%d%H%M%S'))\n","\n","    full_df['idgreen'] = full_df['idgreen'].str.replace(':','')\n","\n","    full_df = full_df[['idgreen',\n","    'lpep_pickup_datetime',\n","    'PULocationID',\n","    'DOLocationID']]\n","\n","    return full_df\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Template code for testing the output of the block.\n","    \"\"\"\n","    assert output is not None, 'The output is undefined'"],"metadata":{"id":"wwDSepttUQg1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"oKtbHJecT8Um"}},{"cell_type":"code","source":["from mage_ai.settings.repo import get_repo_path\n","from mage_ai.io.bigquery import BigQuery\n","from mage_ai.io.config import ConfigFileLoader\n","from pandas import DataFrame\n","from os import path\n","import io\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","\n","if 'data_exporter' not in globals():\n","    from mage_ai.data_preparation.decorators import data_exporter\n","\n","\n","@data_exporter\n","def export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n","    \"\"\"\n","    Template for exporting data to a BigQuery warehouse.\n","    Specify your configuration settings in 'io_config.yaml'.\n","\n","    Docs: https://docs.mage.ai/design/data-loading#bigquery\n","    \"\"\"\n","    table_name = 'black_taxis'\n","    table_id = f'nyc-taxis-project.new_york_transport_project.{table_name}'\n","\n","    config_path = path.join(get_repo_path(), 'io_config.yaml')\n","    config_profile = 'default'\n","\n","    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n","        df,\n","        table_id,\n","        if_exists='replace',  # Specify resolution policy if table name already exists\n","    )"],"metadata":{"id":"MxD4q3_wT37o"},"execution_count":null,"outputs":[]}]}