{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["UVKBHvkWd4ET","jGSUubW8dmu6","6Mc_1_NwlJn0","-51Z7_NqlN4Y","mTCn2eIFlZ9L","WUkeNRJAlfLF","KTgUbS-HlkDL","SSHiu6Fll1mM","987IZb0Ol_jX"],"mount_file_id":"1BQ8Gj9b646Y1_0JdhrNv0q0dEKteb6WK","authorship_tag":"ABX9TyOqlXE5Y5fT4XM/a9/V3dUc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Bloques de cargas incrementales Mage (Triggers)"],"metadata":{"id":"DueENxh6kwig"}},{"cell_type":"markdown","source":["## json_key_admin_cloud_storage.json"],"metadata":{"id":"UVKBHvkWd4ET"}},{"cell_type":"code","source":["{\n","  \"type\": \"service_account\",\n","  \"project_id\": \"nyc-taxis-project\",\n","  \"private_key_id\": \"83f5394cd32e9087e5a3cd3ca1ab3eedcdf3e805\",\n","  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCtgUjZmV029hyN\\n2gUS68vEbKLBsktMHUlr86g4xYAXdUl1d67Cn0LQk6hpjIC7zTbm+3lZN9dUSlsP\\nFn+Ld6J+SBgWDw+uqMnuOkep79nwHLgZUznfPQMmxPf6pLkj0SViszTUG1v5dTM1\\naLSMo9USz9N/vSsRMmAICd2J1U9jRY3mjhaacmudtikoyo7I8RhP5d1WvTmo5/sY\\nivy01OFx6NVfiBZZuQZW32ri1xnASNtCwVuSHw8PuQxiDh+Gu7eu1NmAuiTGixJ/\\n9GssjVQvvmSEgFZ1qKdfYhJ9tESj6pGbzXINlK9nkuwQb/tFxY/sGGgAGKdJkjGN\\n058FRkRRAgMBAAECggEAC5ZQf+241rCRgByxDY4vRc8ugGqQkkT7cAjt8VqpPwfP\\nb9eeFXPO4cCWtPPT7hbapYriVAXBbOjpU2TggDctZCsa5rA7TyIJGE6OligFQT6d\\nwd03xnOb6AUr/XJuRdjUGxcWPlsUZaDA2n6DXkgbtP2ppYSD4YpgdPD5uViETIAO\\nvHHjhZz7sWnNluGtvDGkmt6Y6TVJUrnInDe4avzNdDWQ9MwjLeBfJF2yjqnF9ysW\\nsSDkNNOvG6FuVwtPLKKJVCUZQYJhXmSnc/SU9xt9xyc22zEykrWQ32Xr1KCutbO/\\nEQY/I0mEmCTnPuKi8b7MQQ6tLnTtIpipeDDJz99CdQKBgQDYzeVWXctKhClqqAg5\\ndtP8WS1NOHibBAbXwQa2GOCRQvnc/MIooc5F0o9XmzdMhRxhsqvs0MkK6mx4qsTW\\nKJsec1NDxd61TCpPHTy4Y9VJQhJF1jcZZBBqvQIUT3nSCKC9MYdMT2SsBU6v8hmv\\nta32zANNtEDJm9rHSF0Snhf7jQKBgQDM32slt+/NCjI2+AGg00iJLh1XSoT8slr4\\nNGreQ4/Mz+YzmejzjVUpFawKxwq66YvpMdIeiWzU2h8WYtTRhesF5d162NckvWv8\\n4+/AfTWwFWcZnLiW9cYtOVodpVRiHnExahhH9UbgHnYL2L1lNBJkejTR72vbbOQk\\nZeGWLR/Y1QKBgA9mOIb07I6jaomv272xKgd/kg8tFqv0EkRa6o4eoDzRJAxFS8k1\\nDQC6nwHa/YeK4OQzm7Cbd9w1oHTFtGt9wN7d1Ck5McNw4IR0EC8MsEae0ctyFsHP\\n8LaULESjDIUVpc0qanDKJt1tFxS6PobTgcLI5OctCXkIf/attiavYga1AoGBALzt\\nO03pNYaL5Iq0YiwBK17127Dn04OEpkdFb1fQTge9oLftMtmGeAYPKjw2GTsMrC07\\n+FyYtngDPa/TLBabvIQP2hKzbJNA6ximyi1kSeI7mnwP/lmYBE0oGmZEVHC3SgMb\\np5CUK+v5qzp5gQ6W2m82EweWFoye+jRmj4WBz2CRAoGBAIBo8qkwxGFW4FghsbkQ\\nNiVFcOVYfpAgJzAynYZT18thsAOdfeQlDue2JkivHphxMMZ1KG0Y++x5IwsPxtFn\\nWRjH3mw7oXgvuBJw8Yj4F7N9viRkaqk786GnYovBr4JFlH92J9DThOPduOHBShXt\\n5/OU0eQ2lvqP0KJ1IGhetkz3\\n-----END PRIVATE KEY-----\\n\",\n","  \"client_email\": \"taxis-nyc-project@nyc-taxis-project.iam.gserviceaccount.com\",\n","  \"client_id\": \"101340537597330507674\",\n","  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n","  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n","  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n","  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/taxis-nyc-project%40nyc-taxis-project.iam.gserviceaccount.com\",\n","  \"universe_domain\": \"googleapis.com\"\n","}"],"metadata":{"id":"87Vxqi_pd9tL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## io_config.yaml"],"metadata":{"id":"jGSUubW8dmu6"}},{"cell_type":"code","source":["version: 0.1.1\n","default:\n","  # Default profile created for data IO access.\n","  # Add your credentials for the source you use, and delete the rest.\n","  # AWS\n","  AWS_ACCESS_KEY_ID: \"{{ env_var('AWS_ACCESS_KEY_ID') }}\"\n","  AWS_SECRET_ACCESS_KEY: \"{{ env_var('AWS_SECRET_ACCESS_KEY') }}\"\n","  AWS_SESSION_TOKEN: session_token (Used to generate Redshift credentials)\n","  AWS_REGION: region\n","  # Azure\n","  AZURE_CLIENT_ID: \"{{ env_var('AZURE_CLIENT_ID') }}\"\n","  AZURE_CLIENT_SECRET: \"{{ env_var('AZURE_CLIENT_SECRET') }}\"\n","  AZURE_STORAGE_ACCOUNT_NAME: \"{{ env_var('AZURE_STORAGE_ACCOUNT_NAME') }}\"\n","  AZURE_TENANT_ID: \"{{ env_var('AZURE_TENANT_ID') }}\"\n","  # Clickhouse\n","  CLICKHOUSE_DATABASE: default\n","  CLICKHOUSE_HOST: host.docker.internal\n","  CLICKHOUSE_INTERFACE: http\n","  CLICKHOUSE_PASSWORD: null\n","  CLICKHOUSE_PORT: 8123\n","  CLICKHOUSE_USERNAME: null\n","  # Druid\n","  DRUID_HOST: hostname\n","  DRUID_PASSWORD: password\n","  DRUID_PATH: /druid/v2/sql/\n","  DRUID_PORT: 8082\n","  DRUID_SCHEME: http\n","  DRUID_USER: user\n","  # Google\n","  GOOGLE_SERVICE_ACC_KEY:\n","    type: service_account\n","    \"project_id\": \"nyc-taxis-project\"\n","    \"private_key_id\": \"83f5394cd32e9087e5a3cd3ca1ab3eedcdf3e805\"\n","    \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQCtgUjZmV029hyN\\n2gUS68vEbKLBsktMHUlr86g4xYAXdUl1d67Cn0LQk6hpjIC7zTbm+3lZN9dUSlsP\\nFn+Ld6J+SBgWDw+uqMnuOkep79nwHLgZUznfPQMmxPf6pLkj0SViszTUG1v5dTM1\\naLSMo9USz9N/vSsRMmAICd2J1U9jRY3mjhaacmudtikoyo7I8RhP5d1WvTmo5/sY\\nivy01OFx6NVfiBZZuQZW32ri1xnASNtCwVuSHw8PuQxiDh+Gu7eu1NmAuiTGixJ/\\n9GssjVQvvmSEgFZ1qKdfYhJ9tESj6pGbzXINlK9nkuwQb/tFxY/sGGgAGKdJkjGN\\n058FRkRRAgMBAAECggEAC5ZQf+241rCRgByxDY4vRc8ugGqQkkT7cAjt8VqpPwfP\\nb9eeFXPO4cCWtPPT7hbapYriVAXBbOjpU2TggDctZCsa5rA7TyIJGE6OligFQT6d\\nwd03xnOb6AUr/XJuRdjUGxcWPlsUZaDA2n6DXkgbtP2ppYSD4YpgdPD5uViETIAO\\nvHHjhZz7sWnNluGtvDGkmt6Y6TVJUrnInDe4avzNdDWQ9MwjLeBfJF2yjqnF9ysW\\nsSDkNNOvG6FuVwtPLKKJVCUZQYJhXmSnc/SU9xt9xyc22zEykrWQ32Xr1KCutbO/\\nEQY/I0mEmCTnPuKi8b7MQQ6tLnTtIpipeDDJz99CdQKBgQDYzeVWXctKhClqqAg5\\ndtP8WS1NOHibBAbXwQa2GOCRQvnc/MIooc5F0o9XmzdMhRxhsqvs0MkK6mx4qsTW\\nKJsec1NDxd61TCpPHTy4Y9VJQhJF1jcZZBBqvQIUT3nSCKC9MYdMT2SsBU6v8hmv\\nta32zANNtEDJm9rHSF0Snhf7jQKBgQDM32slt+/NCjI2+AGg00iJLh1XSoT8slr4\\nNGreQ4/Mz+YzmejzjVUpFawKxwq66YvpMdIeiWzU2h8WYtTRhesF5d162NckvWv8\\n4+/AfTWwFWcZnLiW9cYtOVodpVRiHnExahhH9UbgHnYL2L1lNBJkejTR72vbbOQk\\nZeGWLR/Y1QKBgA9mOIb07I6jaomv272xKgd/kg8tFqv0EkRa6o4eoDzRJAxFS8k1\\nDQC6nwHa/YeK4OQzm7Cbd9w1oHTFtGt9wN7d1Ck5McNw4IR0EC8MsEae0ctyFsHP\\n8LaULESjDIUVpc0qanDKJt1tFxS6PobTgcLI5OctCXkIf/attiavYga1AoGBALzt\\nO03pNYaL5Iq0YiwBK17127Dn04OEpkdFb1fQTge9oLftMtmGeAYPKjw2GTsMrC07\\n+FyYtngDPa/TLBabvIQP2hKzbJNA6ximyi1kSeI7mnwP/lmYBE0oGmZEVHC3SgMb\\np5CUK+v5qzp5gQ6W2m82EweWFoye+jRmj4WBz2CRAoGBAIBo8qkwxGFW4FghsbkQ\\nNiVFcOVYfpAgJzAynYZT18thsAOdfeQlDue2JkivHphxMMZ1KG0Y++x5IwsPxtFn\\nWRjH3mw7oXgvuBJw8Yj4F7N9viRkaqk786GnYovBr4JFlH92J9DThOPduOHBShXt\\n5/OU0eQ2lvqP0KJ1IGhetkz3\\n-----END PRIVATE KEY-----\\n\"\n","    \"client_email\": \"taxis-nyc-project@nyc-taxis-project.iam.gserviceaccount.com\"\n","    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\"\n","    \"token_uri\": \"https://oauth2.googleapis.com/token\"\n","    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\"\n","    \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/taxis-nyc-project%40nyc-taxis-project.iam.gserviceaccount.com\"\n","  GOOGLE_SERVICE_ACC_KEY_FILEPATH: \"/path/to/your/service/account/key.json\"\n","  GOOGLE_LOCATION: US # Optional\n","  # MongoDB\n","  # Specify either the connection string or the (host, password, user, port) to connect to MongoDB.\n","  MONGODB_CONNECTION_STRING: \"mongodb://{username}:{password}@{host}:{port}/\"\n","  MONGODB_HOST: host\n","  MONGODB_PORT: 27017\n","  MONGODB_USER: user\n","  MONGODB_PASSWORD: password\n","  MONGODB_DATABASE: database\n","  MONGODB_COLLECTION: collection\n","  # MSSQL\n","  MSSQL_DATABASE: database\n","  MSSQL_SCHEMA: schema\n","  MSSQL_DRIVER: \"ODBC Driver 18 for SQL Server\"\n","  MSSQL_HOST: host\n","  MSSQL_PASSWORD: password\n","  MSSQL_PORT: 1433\n","  MSSQL_USER: SA\n","  # MySQL\n","  MYSQL_DATABASE: database\n","  MYSQL_HOST: host\n","  MYSQL_PASSWORD: password\n","  MYSQL_PORT: 3306\n","  MYSQL_USER: root\n","  # PostgresSQL\n","  POSTGRES_CONNECT_TIMEOUT: 10\n","  POSTGRES_DBNAME: postgres\n","  POSTGRES_SCHEMA: public # Optional\n","  POSTGRES_USER: username\n","  POSTGRES_PASSWORD: password\n","  POSTGRES_HOST: hostname\n","  POSTGRES_PORT: 5432\n","  # Redshift\n","  REDSHIFT_SCHEMA: public # Optional\n","  REDSHIFT_DBNAME: redshift_db_name\n","  REDSHIFT_HOST: redshift_cluster_id.identifier.region.redshift.amazonaws.com\n","  REDSHIFT_PORT: 5439\n","  REDSHIFT_TEMP_CRED_USER: temp_username\n","  REDSHIFT_TEMP_CRED_PASSWORD: temp_password\n","  REDSHIFT_DBUSER: redshift_db_user\n","  REDSHIFT_CLUSTER_ID: redshift_cluster_id\n","  REDSHIFT_IAM_PROFILE: default\n","  # Snowflake\n","  SNOWFLAKE_USER: username\n","  SNOWFLAKE_PASSWORD: password\n","  SNOWFLAKE_ACCOUNT: account_id.region\n","  SNOWFLAKE_DEFAULT_WH: null                  # Optional default warehouse\n","  SNOWFLAKE_DEFAULT_DB: null                  # Optional default database\n","  SNOWFLAKE_DEFAULT_SCHEMA: null              # Optional default schema\n","  SNOWFLAKE_PRIVATE_KEY_PASSPHRASE: null      # Optional private key passphrase\n","  SNOWFLAKE_PRIVATE_KEY_PATH: null            # Optional private key path\n","  SNOWFLAKE_ROLE: null                        # Optional role name\n","  SNOWFLAKE_TIMEOUT: null                     # Optional timeout in seconds\n","  # Trino\n","  trino:\n","    catalog: postgresql                       # Change this to the catalog of your choice\n","    host: 127.0.0.1\n","    http_headers:\n","      X-Something: 'mage=power'\n","    http_scheme: http\n","    password: mage1337                        # Optional\n","    port: 8080\n","    schema: core_data\n","    session_properties:                       # Optional\n","      acc01.optimize_locality_enabled: false\n","      optimize_hash_generation: true\n","    source: trino-cli                         # Optional\n","    user: admin\n","    verify: /path/to/your/ca.crt              # Optional\n"],"metadata":{"id":"IQyTE95Idsiy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Green Taxis"],"metadata":{"id":"6Mc_1_NwlJn0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fRsva72kvZO"},"outputs":[],"source":["# Importo librerias necesarias\n","import io\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","from google.cloud import bigquery\n","import json\n","import os\n","import datetime\n","\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'taxis_nyc_project/json_key_admin_cloud_storage.json'\n","\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    taxi = 'green'\n","    trip_data = 'green_tripdata'\n","\n","    # Inicializa el cliente de BigQuery\n","    client = bigquery.Client()\n","\n","    project_name = 'nyc-taxis-project'\n","    dataset_name = 'new_york_transport_project'\n","    table_name = f'{taxi}_taxis'\n","\n","    # Especifica la referencia a la tabla que deseas visualizar\n","    table_ref = client.dataset(dataset_name).table(table_name)\n","    table = client.get_table(table_ref)\n","\n","    query = f\"\"\"\n","        SELECT DATE(pickup_datetime)\n","        FROM `{project_name}.{dataset_name}.{table_name}`\n","        ORDER BY pickup_datetime desc\n","        LIMIT 1;\n","    \"\"\"\n","\n","    query_job = client.query(query)\n","    results = query_job.result()\n","    for row in results:\n","        resultado = row[0]\n","\n","    date_taxis = resultado - datetime.timedelta(days=1)\n","    date_taxis_str = f'{date_taxis.year}-{str(date_taxis.month).zfill(2)}'\n","\n","    url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    parquet_files = []\n","\n","    selected_years = [str(y) for y in list(range(datetime.datetime.strptime(date_taxis_str, '%Y-%m').year,datetime.datetime.strptime(date_taxis_str, '%Y-%m').year+10))]\n","    manhattan_zones = [  4,  12,  13,  24,  41,  42,  43,  45,  48,  50,  68,  74,  75,\n","        79,  87,  88,  90, 100, 103, 104, 105, 107, 113, 114, 116, 120,\n","        125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153,\n","        158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211, 224,\n","        229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246,\n","        249, 261, 262, 263]\n","\n","    for link in soup.find_all('a',href=True):\n","        if 'parquet' in link['href']:\n","            if any(year in link['href'] for year in selected_years):\n","                parquet_files.append(link['href'])\n","\n","    taxis_data = []\n","    for link in parquet_files:\n","        if trip_data in link:\n","            taxis_data.append(link)\n","\n","    taxis_data = sorted([link.rstrip().lstrip() for link in taxis_data], reverse=True)\n","\n","    dataframes = []\n","    for link in sorted(taxis_data, reverse=False):\n","        date_link = link.split('/')[-1].split('_')[-1].replace('.parquet','')\n","        if (date_taxis_str < date_link) == True:\n","            df = pd.read_parquet(link)\n","            df = df.rename(columns={'lpep_pickup_datetime':'pickup_datetime'})\n","            df = df[['pickup_datetime','PULocationID','DOLocationID']]\n","            df = df[df.PULocationID.isin(manhattan_zones) & df.DOLocationID.isin(manhattan_zones)].reset_index(drop=True)\n","            dataframes.append(df)\n","\n","    if len(dataframes) == 0:\n","        return pd.DataFrame()\n","    else:\n","       return pd.concat(dataframes, axis = 0, ignore_index = True)\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Template code for testing the output of the block.\n","    \"\"\"\n","    assert output is not None, 'The output is undefined'"]},{"cell_type":"markdown","source":["## Yellow Taxis"],"metadata":{"id":"-51Z7_NqlN4Y"}},{"cell_type":"code","source":["# Importo librerias necesarias\n","import io\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","from google.cloud import bigquery\n","import json\n","import os\n","import datetime\n","\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'taxis_nyc_project/json_key_admin_cloud_storage.json'\n","\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    taxi = 'yellow'\n","    trip_data = 'yellow_tripdata'\n","\n","    # Inicializa el cliente de BigQuery\n","    client = bigquery.Client()\n","\n","    project_name = 'nyc-taxis-project'\n","    dataset_name = 'new_york_transport_project'\n","    table_name = f'{taxi}_taxis'\n","\n","    # Especifica la referencia a la tabla que deseas visualizar\n","    table_ref = client.dataset(dataset_name).table(table_name)\n","    table = client.get_table(table_ref)\n","\n","    query = f\"\"\"\n","        SELECT DATE(pickup_datetime)\n","        FROM `{project_name}.{dataset_name}.{table_name}`\n","        ORDER BY pickup_datetime desc\n","        LIMIT 1;\n","    \"\"\"\n","\n","    query_job = client.query(query)\n","    results = query_job.result()\n","    for row in results:\n","        resultado = row[0]\n","\n","    date_taxis = resultado - datetime.timedelta(days=1)\n","    date_taxis_str = f'{date_taxis.year}-{str(date_taxis.month).zfill(2)}'\n","\n","    url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    parquet_files = []\n","\n","    selected_years = [str(y) for y in list(range(datetime.datetime.strptime(date_taxis_str, '%Y-%m').year,datetime.datetime.strptime(date_taxis_str, '%Y-%m').year+10))]\n","    manhattan_zones = [  4,  12,  13,  24,  41,  42,  43,  45,  48,  50,  68,  74,  75,\n","        79,  87,  88,  90, 100, 103, 104, 105, 107, 113, 114, 116, 120,\n","        125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153,\n","        158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211, 224,\n","        229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246,\n","        249, 261, 262, 263]\n","\n","    for link in soup.find_all('a',href=True):\n","        if 'parquet' in link['href']:\n","            if any(year in link['href'] for year in selected_years):\n","                parquet_files.append(link['href'])\n","\n","    taxis_data = []\n","    for link in parquet_files:\n","        if trip_data in link:\n","            taxis_data.append(link)\n","\n","    taxis_data = sorted([link.rstrip().lstrip() for link in taxis_data], reverse=True)\n","\n","    dataframes = []\n","    for link in sorted(taxis_data, reverse=False):\n","        date_link = link.split('/')[-1].split('_')[-1].replace('.parquet','')\n","        if (date_taxis_str < date_link) == True:\n","            df = pd.read_parquet(link)\n","            df = df.rename(columns={'tpep_pickup_datetime':'pickup_datetime'})\n","            df = df[['pickup_datetime','PULocationID','DOLocationID']]\n","            df = df[df.PULocationID.isin(manhattan_zones) & df.DOLocationID.isin(manhattan_zones)].reset_index(drop=True)\n","            dataframes.append(df)\n","\n","    if len(dataframes) == 0:\n","        return pd.DataFrame()\n","    else:\n","       return pd.concat(dataframes, axis = 0, ignore_index = True)\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Template code for testing the output of the block.\n","    \"\"\"\n","    assert output is not None, 'The output is undefined'"],"metadata":{"id":"kTk0KfLulPAX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Grey Taxis"],"metadata":{"id":"mTCn2eIFlZ9L"}},{"cell_type":"code","source":["# Importo librerias necesarias\n","import io\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","from google.cloud import bigquery\n","import json\n","import os\n","import datetime\n","\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'taxis_nyc_project/json_key_admin_cloud_storage.json'\n","\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    taxi = 'grey'\n","    trip_data = 'fhv_tripdata'\n","\n","    # Inicializa el cliente de BigQuery\n","    client = bigquery.Client()\n","\n","    project_name = 'nyc-taxis-project'\n","    dataset_name = 'new_york_transport_project'\n","    table_name = f'{taxi}_taxis'\n","\n","    # Especifica la referencia a la tabla que deseas visualizar\n","    table_ref = client.dataset(dataset_name).table(table_name)\n","    table = client.get_table(table_ref)\n","\n","    query = f\"\"\"\n","        SELECT DATE(pickup_datetime)\n","        FROM `{project_name}.{dataset_name}.{table_name}`\n","        ORDER BY pickup_datetime desc\n","        LIMIT 1;\n","    \"\"\"\n","\n","    query_job = client.query(query)\n","    results = query_job.result()\n","    for row in results:\n","        resultado = row[0]\n","\n","    date_taxis = resultado - datetime.timedelta(days=1)\n","    date_taxis_str = f'{date_taxis.year}-{str(date_taxis.month).zfill(2)}'\n","\n","    url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    parquet_files = []\n","\n","    selected_years = [str(y) for y in list(range(datetime.datetime.strptime(date_taxis_str, '%Y-%m').year,datetime.datetime.strptime(date_taxis_str, '%Y-%m').year+10))]\n","    manhattan_zones = [  4,  12,  13,  24,  41,  42,  43,  45,  48,  50,  68,  74,  75,\n","        79,  87,  88,  90, 100, 103, 104, 105, 107, 113, 114, 116, 120,\n","        125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153,\n","        158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211, 224,\n","        229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246,\n","        249, 261, 262, 263]\n","\n","    for link in soup.find_all('a',href=True):\n","        if 'parquet' in link['href']:\n","            if any(year in link['href'] for year in selected_years):\n","                parquet_files.append(link['href'])\n","\n","    taxis_data = []\n","    for link in parquet_files:\n","        if trip_data in link:\n","            taxis_data.append(link)\n","\n","    taxis_data = sorted([link.rstrip().lstrip() for link in taxis_data], reverse=True)\n","\n","    dataframes = []\n","    for link in sorted(taxis_data, reverse=False):\n","        date_link = link.split('/')[-1].split('_')[-1].replace('.parquet','')\n","        if (date_taxis_str < date_link) == True:\n","            df = pd.read_parquet(link)\n","            df = df.rename(columns={'PUlocationID':'PULocationID', 'DOlocationID':'DOLocationID'})\n","            df = df[['pickup_datetime','PULocationID','DOLocationID']]\n","            df = df[df.PULocationID.isin(manhattan_zones) & df.DOLocationID.isin(manhattan_zones)].reset_index(drop=True)\n","            dataframes.append(df)\n","\n","    if len(dataframes) == 0:\n","        return pd.DataFrame()\n","    else:\n","       return pd.concat(dataframes, axis = 0, ignore_index = True)\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Template code for testing the output of the block.\n","    \"\"\"\n","    assert output is not None, 'The output is undefined'"],"metadata":{"id":"MppQddIDlcPe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Black Taxis"],"metadata":{"id":"WUkeNRJAlfLF"}},{"cell_type":"code","source":["# Importo librerias necesarias\n","import io\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","from google.cloud import bigquery\n","import json\n","import os\n","import datetime\n","\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'taxis_nyc_project/json_key_admin_cloud_storage.json'\n","\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    taxi = 'black'\n","    trip_data = 'fhvhv_tripdata'\n","\n","    # Inicializa el cliente de BigQuery\n","    client = bigquery.Client()\n","\n","    project_name = 'nyc-taxis-project'\n","    dataset_name = 'new_york_transport_project'\n","    table_name = f'{taxi}_taxis'\n","\n","    # Especifica la referencia a la tabla que deseas visualizar\n","    table_ref = client.dataset(dataset_name).table(table_name)\n","    table = client.get_table(table_ref)\n","\n","    query = f\"\"\"\n","        SELECT DATE(pickup_datetime)\n","        FROM `{project_name}.{dataset_name}.{table_name}`\n","        ORDER BY pickup_datetime desc\n","        LIMIT 1;\n","    \"\"\"\n","\n","    query_job = client.query(query)\n","    results = query_job.result()\n","    for row in results:\n","        resultado = row[0]\n","\n","    date_taxis = resultado - datetime.timedelta(days=1)\n","    date_taxis_str = f'{date_taxis.year}-{str(date_taxis.month).zfill(2)}'\n","\n","    url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, \"html.parser\")\n","    parquet_files = []\n","\n","    selected_years = [str(y) for y in list(range(datetime.datetime.strptime(date_taxis_str, '%Y-%m').year,datetime.datetime.strptime(date_taxis_str, '%Y-%m').year+10))]\n","    manhattan_zones = [  4,  12,  13,  24,  41,  42,  43,  45,  48,  50,  68,  74,  75,\n","        79,  87,  88,  90, 100, 103, 104, 105, 107, 113, 114, 116, 120,\n","        125, 127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153,\n","        158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211, 224,\n","        229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246,\n","        249, 261, 262, 263]\n","\n","    for link in soup.find_all('a',href=True):\n","        if 'parquet' in link['href']:\n","            if any(year in link['href'] for year in selected_years):\n","                parquet_files.append(link['href'])\n","\n","    taxis_data = []\n","    for link in parquet_files:\n","        if trip_data in link:\n","            taxis_data.append(link)\n","\n","    taxis_data = sorted([link.rstrip().lstrip() for link in taxis_data], reverse=True)\n","\n","    dataframes = []\n","    for link in sorted(taxis_data, reverse=False):\n","        date_link = link.split('/')[-1].split('_')[-1].replace('.parquet','')\n","        if (date_taxis_str < date_link) == True:\n","            df = pd.read_parquet(link)\n","            df = df[['pickup_datetime','PULocationID','DOLocationID']]\n","            df = df[df.PULocationID.isin(manhattan_zones) & df.DOLocationID.isin(manhattan_zones)].reset_index(drop=True)\n","            dataframes.append(df)\n","\n","    if len(dataframes) == 0:\n","        return pd.DataFrame()\n","    else:\n","       return pd.concat(dataframes, axis = 0, ignore_index = True)\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Template code for testing the output of the block.\n","    \"\"\"\n","    assert output is not None, 'The output is undefined'"],"metadata":{"id":"XrHz5h4Clg9U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## OpenWeather API"],"metadata":{"id":"KTgUbS-HlkDL"}},{"cell_type":"code","source":["# Importo librerias necesarias\n","import pandas as pd\n","import json\n","from google.cloud import bigquery\n","import os\n","from datetime import datetime, timedelta\n","import requests\n","from bs4 import BeautifulSoup\n","from geopy.geocoders import Nominatim\n","from pandas.errors import EmptyDataError\n","import warnings\n","import io\n","\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'taxis_nyc_project/json_key_admin_cloud_storage.json'\n","\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    client = bigquery.Client()\n","\n","    api_key = 'e49a90e730e178434b65ddc9689cbf9d'\n","    dfs = []\n","\n","    zones_names = ['Alphabet City', 'Battery Park', 'Battery Park City', 'Bloomingdale', 'Central Harlem', 'Central Harlem North', 'Central Park', 'Chinatown', 'Clinton East', 'Clinton West', 'East Chelsea', 'East Harlem South', 'East Village', 'Financial District South', 'Flatiron', 'Garment District', \"Governor's Island/Ellis Island/Liberty Island\", 'Gramercy', 'Greenwich Village South', 'Hamilton Heights', 'Highbridge Park', 'Hudson Sq', 'Inwood', 'Inwood Hill Park', 'Kips Bay', 'Lenox Hill East', 'Lincoln Square East', 'Lincoln Square West', 'Lower East Side', 'Manhattan Valley', 'Manhattanville', 'Marble Hill', 'Midtown Center', 'Midtown East', 'Midtown North', 'Midtown South', 'Morningside Heights', 'Murray Hill', 'Randalls Island', 'Roosevelt Island', 'Seaport', 'SoHo', 'Sutton Place/Turtle Bay North', 'Times Sq/Theatre District', 'Two Bridges/Seward Park', 'Union Sq', 'Upper East Side South', 'Upper West Side South', 'Washington Heights South', 'West Chelsea/Hudson Yards', 'West Village', 'World Trade Center', 'Yorkville East', 'East Harlem', 'Financial District', 'Governors Island', 'Ellis Island', 'Liberty Island', 'Greenwich Village', 'Lenox Hill West', 'Little Italy', 'NoLiTa', 'Meatpacking', 'West Village West', 'Midtown Center', 'Penn Station', 'Madison Sq West', 'Stuy Town', 'Peter Cooper Village', 'Sutton Place', 'Turtle Bay', 'TriBeCa', 'Civic Center', 'Two Bridges', 'Seward Park', 'UN', 'Upper East Side', 'Upper West Side', 'Washington Heights', 'Yorkville West']\n","    locations_ids = [4, 12, 13, 24, 41, 42, 43, 45, 48, 50, 68, 75, 79, 88, 90, 100, 104, 107, 114, 116, 120, 125, 127, 128, 137, 140, 142, 143, 148, 151, 152, 153, 161, 162, 163, 164, 166, 170, 194, 202, 209, 211, 229, 230, 232, 234, 237, 239, 244, 246, 249, 261, 262, 74, 87, 103, 103, 103, 113, 141, 144, 144, 158, 158, 161, 186, 186, 224, 224, 229, 229, 231, 231, 232, 232, 233, 236, 238, 243, 263]\n","\n","    project_name = 'nyc-taxis-project'\n","    dataset_name = 'new_york_transport_project'\n","    table_name = 'carbon_monoxide'\n","\n","    # Especifica la referencia a la tabla que deseas visualizar\n","    table_ref = client.dataset(dataset_name).table(table_name)  # Reemplaza 'dataset_id' y 'table_id' con los valores reales\n","\n","    # Obtiene la tabla desde BigQuery\n","    table = client.get_table(table_ref)\n","\n","    query = f\"\"\"\n","        SELECT date, time\n","        FROM `{project_name}.{dataset_name}.{table_name}`\n","        ORDER BY date DESC, time DESC\n","        LIMIT 1;\n","    \"\"\"\n","\n","    query_job = client.query(query)\n","    results = query_job.result()\n","    for row in results:\n","        dt_format = \"%Y-%m-%d %H:%M:%S\"  # Formato de fecha y hora\n","        dt_str = row.date.strftime(\"%Y-%m-%d\") + \" \" + row.time.strftime(\"%H:%M:%S\")  # Convertir datetime a str\n","        recent_date = datetime.strptime(dt_str, dt_format)\n","        print(recent_date)\n","\n","    # Inicializar el geolocalizador de Nominatim\n","    geolocator = Nominatim(user_agent=\"my_geocoder\", timeout=10)\n","    fails_zones = {\n","        'Zone':[],\n","        'IdZone':[]\n","    }\n","\n","    start_date = recent_date.strftime(\"%Y-%m-%d\")\n","    start_date_datetime = datetime.strptime(start_date, \"%Y-%m-%d\")\n","    start_date_unix = start_date_datetime.timestamp()\n","\n","    end_date = f'{datetime.now().date()}'\n","    end_date_datetime = datetime.strptime(end_date, \"%Y-%m-%d\")\n","    end_date_unix = end_date_datetime.timestamp()\n","\n","    # Obtener las coordenadas de cada ciudad/barrio\n","    for index, zone in enumerate(zones_names):\n","        location = geolocator.geocode(zone + \", Manhattan, New York, USA\")\n","        # URL de la API con tus valores de latitud, longitud y API key\n","\n","        if location is None:\n","            fails_zones['Zone'].append(zone)\n","            fails_zones['IdZone'].append(locations_ids[index])\n","            continue\n","        else:\n","            lat = location.latitude\n","            lon = location.longitude\n","            start = int(start_date_unix)\n","            end = int(end_date_unix)\n","\n","            url = f\"http://api.openweathermap.org/data/2.5/air_pollution/history?lat={lat}&lon={lon}&start={start}&end={end}&appid={api_key}\"\n","\n","            # Realizar la solicitud a la API\n","            response = requests.get(url)\n","\n","            data = response.json()\n","            clean_data = {\n","                'unix_date': [],\n","                'co': [],\n","                'pm2.5': [],\n","                'pm10': []\n","                }\n","\n","            for dictt in data['list']:\n","                clean_data['unix_date'].append(dictt['dt'])\n","                clean_data['co'].append(dictt['components']['co'])\n","                clean_data['pm2.5'].append(dictt['components']['pm2_5'])\n","                clean_data['pm10'].append(dictt['components']['pm10'])\n","\n","            df = pd.DataFrame(clean_data)\n","            df['latitude'] = lat\n","            df['longitude'] = lon\n","            df['datetime'] = df['unix_date'].apply(lambda x: datetime.fromtimestamp(x))\n","            df['date'] = df['unix_date'].apply(lambda x: datetime.fromtimestamp(x).date())\n","            df['time'] = df['unix_date'].apply(lambda x: datetime.fromtimestamp(x).time())\n","            df['zone'] = zone\n","            df['idzone'] = locations_ids[index]\n","            df['date'] = pd.to_datetime(df['date'])\n","            df['year'] = df['date'].dt.year\n","            df['month'] = df['date'].dt.month\n","            df['day'] = df['date'].dt.day\n","            df['quarter'] = df['date'].dt.quarter\n","\n","            df.drop(columns=['unix_date'], inplace = True)\n","            dfs.append(df)\n","\n","\n","    df = pd.concat(dfs, ignore_index=True)\n","\n","    df = df[['datetime',\n","        'date', 'time', 'year', 'quarter', 'month', 'zone', 'idzone',\n","        'latitude', 'longitude', 'co', 'pm2.5', 'pm10'\n","    ]]\n","\n","    df = df[(df.datetime.isna() == False) & (df.datetime > recent_date)].drop_duplicates().reset_index(drop=True)\n","    df = df.drop(columns=['datetime']).rename(columns={'pm2.5':'pm2_5'})\n","    df = df.sort_values(by=['date','time'], ascending=False).drop_duplicates().reset_index(drop=True)\n","\n","    return df\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Template code for testing the output of the block.\n","    \"\"\"\n","    assert output is not None, 'The output is undefined'"],"metadata":{"id":"42zt1nwClmVY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Noise"],"metadata":{"id":"SSHiu6Fll1mM"}},{"cell_type":"code","source":["import io\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","!pip3 install google-cloud-bigquery\n","from google.cloud import bigquery\n","\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'json_key_admin_cloud_storage.json'\n","\n","if 'data_loader' not in globals():\n","    from mage_ai.data_preparation.decorators import data_loader\n","if 'test' not in globals():\n","    from mage_ai.data_preparation.decorators import test\n","\n","\n","@data_loader\n","def load_data_from_api(*args, **kwargs):\n","    \"\"\"\n","    Template for loading data from API\n","    \"\"\"\n","    # Inicializa el cliente de BigQuery\n","    client = bigquery.Client()\n","\n","    project_name = 'nyc-taxis-project'\n","    dataset_name = 'new_york_transport_project'\n","    table_name = 'noise_pollution'\n","    # Especifica la referencia a la tabla que deseas visualizar\n","    table_ref = client.dataset(dataset_name).table(table_name)  # Reemplaza 'dataset_id' y 'table_id' con los valores reales\n","\n","    # Obtiene la tabla desde BigQuery\n","    table = client.get_table(table_ref)\n","\n","    query = f\"\"\"\n","    SELECT DATE(lpep_pickup_datetime)\n","    FROM `{project_name}.{dataset_name}.{table_name}`\n","    ORDER BY lpep_pickup_datetime desc limit 1;\n","    \"\"\"\n","    query_job = client.query(query)\n","    results = query_job.result()\n","    for row in results:\n","        print(row[0])\n","\n","    return full_df\n","\n","@test\n","def test_output(output, *args) -> None:\n","    \"\"\"\n","    Template code for testing the output of the block.\n","    \"\"\"\n","    assert output is not None, 'The output is undefined'"],"metadata":{"id":"VIeSG6YXl2XX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bloque generico de exportación a BigQuery"],"metadata":{"id":"987IZb0Ol_jX"}},{"cell_type":"code","source":["import pandas as pd\n","from google.cloud import bigquery\n","\n","if 'data_exporter' not in globals():\n","    from mage_ai.data_preparation.decorators import data_exporter\n","\n","\n","@data_exporter\n","def export_data(df, *args, **kwargs):\n","    \"\"\"\n","    Exports data to some source.\n","\n","    Args:\n","        data: The output from the upstream parent block\n","        args: The output from any additional upstream blocks (if applicable)\n","\n","    Output (optional):\n","        Optionally return any object and it'll be logged and\n","        displayed when inspecting the block run.\n","    \"\"\"\n","    # Specify your data exporting logic here\n","\n","    if len(df) > 1:\n","        # Debes reemplazar 'tu-proyecto' con el nombre de tu proyecto en GCP\n","        client = bigquery.Client(project = 'nyc-taxis-project')\n","\n","        # Especifica el ID del conjunto de datos y el nombre de la tabla en BigQuery\n","        dataset_id = 'new_york_transport_project'\n","        table_id = 'grey_taxis'\n","\n","        # Crea una referencia a la tabla en BigQuery\n","        table_ref = client.dataset(dataset_id).table(table_id)\n","\n","        # Convierte el DataFrame en un formato compatible con BigQuery\n","        job_config = bigquery.LoadJobConfig()\n","        job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n","        job_config.autodetect = True\n","\n","        # Carga los datos del DataFrame a la tabla en BigQuery\n","        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n","        job.result()  # Espera a que se complete el proceso de carga\n","\n","        print(f\"Registros cargados en la tabla {table_id} - {job.output_rows} registros\")\n","    else:\n","        print('No hay registros nuevos para cargar en la tabla.')"],"metadata":{"id":"d0a3qwNGmCxu"},"execution_count":null,"outputs":[]}]}